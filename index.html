<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Ching-I (Isabella) Huang</title>
  <meta name="description" content="Isabella's CV" />
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700,200" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link href="css/aos.css" rel="stylesheet" />
  <link href="css/bootstrap.min.css" rel="stylesheet" />
  <link href="styles/main.css" rel="stylesheet" />
  <script src="https://kit.fontawesome.com/5c88c749c7.js" crossorigin="anonymous"></script>
</head>

<body id="top">
  <header>
    <div class="profile-page sidebar-collapse">
      <nav class="navbar navbar-expand-lg fixed-top navbar-transparent bg-primary" color-on-scroll="400">
        <div class="container">
          <div class="navbar-translate">
            <a class="navbar-brand" href="#" rel="tooltip">Ching-I (Isabella) Huang</a>
            <button 
              class="navbar-toggler navbar-toggler" 
              type="button" data-toggle="collapse" 
              data-target="#navigation"
              aria-controls="navigation" 
              aria-expanded="false" 
              aria-label="Toggle navigation">
              <span class="navbar-toggler-bar bar1"></span>
              <span class="navbar-toggler-bar bar2"></span>
              <span class="navbar-toggler-bar bar3"></span>
            </button>
          </div>

          <div class="collapse navbar-collapse justify-content-end" id="navigation">
            <ul class="navbar-nav">
              <li class="nav-item">
                <a class="nav-link smooth-scroll" href="#about">About</a>
              </li>
              <!-- <li class="nav-item">
                <a class="nav-link smooth-scroll" href="#skill">Skills</a>
              </li> -->
              <li class="nav-item">
                <a class="nav-link smooth-scroll" href="#publications">Publications
                </a>
              </li>
              <!-- <li class="nav-item">
                <a class="nav-link smooth-scroll" href="#projects">Projects
                </a>
              </li> -->
              <li class="nav-item">
                <a class="nav-link smooth-scroll" href="#contact">Contact</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
  </header>

  <div class="page-content">
    <div class="profile-page">
      <div class="wrapper">
        <div class="page-header page-header-small">
          <div 
            class="page-header-image"   
            data-parallax="true" 
            style="background-image: url('images/background.png')">
          </div>
          <div class="container">
            <div class="content-center">

              <div class="h2 title">Ching-I (Isabella) Huang</div>
              <p class="category text-white">Robotics, Robot Learning, VR</p>
            </div>
          </div>
          <div class="section">
            <div class="container">
              <div class="button-container">
                <a class="btn btn-default btn-round btn-lg btn-icon"
                  href="https://drive.google.com/file/d/1pgxGCClP2rFQY2G4Mk4YlGiqjL1J11-8/view?usp=sharing"
                  rel="tooltip" title="Curriculum Vitae"><i class="ai ai-cv-square ai-2x"
                    style="padding-top: 0.35cm"></i></a>
                <a class="btn btn-default btn-round btn-lg btn-icon"
                  href="https://www.linkedin.com/in/ching-i-h-6696a9220/" rel="tooltip" title="LinkedIn"><i
                    class="fa fa-linkedin"></i></a>
                <a class="btn btn-default btn-round btn-lg btn-icon" href="https://github.com/cihuang123" rel="tooltip"
                  title="Github"><i class="fa fa-github"></i></a>
                <a class="btn btn-default btn-round btn-lg btn-icon"
                  href="https://scholar.google.com/citations?hl=en&user=PQfRXVYAAAAJ" rel="tooltip"
                  title="Google Scholar">
                  <i class="ai ai-google-scholar ai-2x" style="padding-top: 0.35cm"></i>
                </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="section" id="about">
      <div class="container">
        <div class="card" data-aos="fade-up" data-aos-offset="10">
          <div class="row">
            <div class="col-lg-4 col-md-12">
              <div class="card-body">
                <br />
                <img src="images/isabella.jpg" alt="Image" width="160" style="
                      display: block;
                      margin-left: auto;
                      margin-right: auto;
                      border-radius: 20px;
                    " />
              </div>
            </div>
            <div class="col-lg-8 col-md-12">
              <div class="card-body">
                <div class="h4 mt-0 title">About Me</div>
                <p>
                  Hello! I am Ching-I (Isabella) Huang, a fourth-year Ph.D. student enrolled at
                  the Institute of Electrical and Control Engineering, National Yang Ming Chiao Tung University in
                  Taiwan.
                  Under the supervision of Prof. Hsueh-Cheng Wang,
                  I am currently engaged in research focusing on <b>robot-learning approaches</b> and <b>virtual reality
                    technologies</b>
                  aimed at improving human-robot interaction.
                  My research interests primarily center around <b>Human-Robot Teaming</b> and <b>VR Teleoperation</b>.
                </p>
                <p>
                  I have participated in the <b> DARPA SubT Challenge </b> in 2020 (communication lead) and
                  the <b>RobotX Maritime Challenge</b> in 2022 (3rd place, perception lead).
                  Additionally, in 2023, I had the opportunity to visit Prof. Lap-Fai (Craig) Yu's research group at
                  George Mason University,
                  where I collaborated on a project concerning optimal theories for gaming using augmented reality.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <br><br><br><br>

    <div class="section" id="publications">
      <div class="container">
        <div class="h4 text-center mb-4 title">Publications</div>

        <div class="card" id="vr-navigation">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/projects/vr-navigation.html">
                  <figure class="cc-effect">
                    <img src="project/vr-navigation/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>
                    An Evaluation Framework of Human-Robot Teaming
                    for Navigation among Movable Obstacles via Virtual Reality-based Interactions</b>
                </p>
                <p class="category">IEEE Robotics and Automation Letters - 2024</p>
                <p>
                  <b>Ching-I Huang</b>, Sun-Fu Chou, Li-Wei Liou, Nathan Alan Moy,
                  Chi-Ruei Wang, Hsueh-Cheng Wang, Charles Ahn, Chun-Ting Huang and Lap-Fai Yu
                </p>
                <p>
                  We present a framework that enables intuitive oversight of a robot team through immersive virtual
                  reality (VR) visualizations.
                  The framework simplifies the management of complex navigation among movable obstacles (NAMO) tasks,
                  such as search-and- rescue tasks.
                  Specifically, the framework integrates a simulation of the environment with robot sensor data in VR to
                  facilitate operator navigation,
                  enhance robot positioning, and greatly improve operator situational awareness.
                  The framework can also boost mission efficiency by seamlessly incorporating autonomous navigation
                  algorithms,
                  including NAMO algorithms, to reduce detours and operator workload.
                </p>

                <p>
                  The framework is effective for operating in both simulated and real scenarios and is thus ideal for
                  training or evaluating autonomous navigation algorithms.
                  To validate the framework, we conducted user studies (N = 53) on the basis of the DARPA SubT
                  Challenge’s search-and-rescue missions.
                </p>

                <a class="btn btn-primary" href="https://vimeo.com/865390496">Video
                </a>
                <a class="btn btn-primary" href="https://github.com/cihuang123/framwork-unity">Unity</a>
                <a class="btn btn-primary" href="https://github.com/cihuang123/framework-ros">ROS</a>
                <!-- <a class="btn btn-primary" href="project/vr_navigation.html"
                    >More...</a> -->
              </div>
            </div>
          </div>
        </div>

        <div class="card" id="fedhanet">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/projects/fed-hanet.html">
                  <figure class="cc-effect">
                    <img src="project/fedhanet/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>
                    Fed-hanet:Federated visual grasping learning for human robot handovers</b>
                </p>
                <p class="category">
                  IEEE Robotics and Automation Letters - 2023
                </p>
                <p>
                  <b>Ching-I Huang</b>, Yu-Yen Huang, Jie-Xin Liu, Yu-Ting Ko,
                  Hsueh-Cheng Wang, Kuang-Hsing Chiang and Lap-Fai Yu
                </p>
                <p>
                  Our Fed-HANet system has made significant strides in object-agnostic end-to-end planar grasping,
                  offering capabilities of up to six degrees of freedom (DoF)
                  while maintaining respect for the privacy and personal space of human counterparts.
                  Through experiments, our Fed-HANet system demonstrated accuracy levels comparable to centralized
                  non-privacy-preserving systems,
                  surpassing baseline methods reliant on fine-tuning. Additionally, we investigated the efficacy of a
                  depth-only approach,
                  comparing its performance to a state-of-the-art method, yet highlighting the superiority of RGB inputs
                  for enhanced grasp success.
                  Furthermore, we conducted a user study involving 12 participants to evaluate the practical
                  applicability of our proposed system within a robotic framework.
                </p>

                <a class="btn btn-primary" href="https://ieeexplore.ieee.org/abstract/document/10109097/">Paper
                </a>
                <a class="btn btn-primary" href="https://vimeo.com/759768324">Video
                </a>
                <a class="btn btn-primary" href="https://github.com/ARG-NCTU/handover-system">System
                </a>
                <a class="btn btn-primary" href="https://github.com/ARG-NCTU/handover_grasping">Inference
                </a>
                <!-- <a class="btn btn-primary" href="project-fedhanet.html"
                    >More...</a
                  > -->
              </div>
            </div>
          </div>
        </div>

        <div class="card" id="EfficientDets">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/projects/efficientdet.html">
                  <figure class="cc-effect">
                    <img src="project/EfficientDets/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>
                    Towards more efficient efficientdets and real-time marine debris detection</b>
                </p>
                <p class="category">
                  IEEE Robotics and Automation Letters - 2023
                </p>
                <p>
                  Federico Zocco, Tzu-Chieh Lin, <b>Ching-I Huang</b>,
                  Hsueh-Cheng Wang, Mohammad Omar Khyam and Lap-Fai Yu
                </p>
                <p>
                  In our pursuit of enhancing the efficiency of Autonomous Underwater Vehicle (AUV) vision systems for
                  real-time marine debris detection,
                  we have achieved significant advancements through the optimization of state-of-the-art object
                  detectors, specifically focusing on EfficientDets.
                  Our efforts have yielded notable improvements, including a 1.5% increase in Average Precision (AP) on
                  D0, 2.6% on D1, 1.2% on D2, and 1.3% on D3,
                  all achieved without incurring additional GPU latency.
                  Additionally, we have curated and publicly released a specialized dataset tailored for the detection
                  of in-water plastic bags and bottles.
                  Our enhanced EfficientDets models have been rigorously trained on this dataset, alongside integration
                  with two publicly available datasets
                  dedicated to marine debris detection.
                </p>

                <a class="btn btn-primary" href="https://ieeexplore.ieee.org/abstract/document/10044917">Paper
                </a>
                <a class="btn btn-primary" href="https://vimeo.com/794011415">Video
                </a>
                <!-- <a class="btn btn-primary" href="project-fedhanet.html"
              >More...</a
            > -->
              </div>
            </div>
          </div>
        </div>

        <div class="card" id="subtUrban">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/SubT/index.html">
                  <figure class="cc-effect">
                    <img src="project/subt-urban/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>
                    A Heterogeneous Unmanned Ground Vehicle and Blimp Robot
                    Team for Search and Rescue using Data-driven Autonomy and
                    Communication-aware Navigation</b>
                </p>
                <p></p>
                <p class="category">
                  Journal of Field Robotics - 2022 <br />
                  DARPA SubT Challenge - Urban Circuit 2020
                </p>
                <p>
                  Chen-Lung Lu*, Jui-Te Huang*, <b>Ching-I Huang</b>, Zi-Yan
                  Liu, Chao-Chun Hsu, Yu-Yen Huang, Siao-Cing Huang, Po-Kai
                  Chang, Zu Lin Ewe, Po-Jui Huang, Po-Lin Li, Bo-Hui Wang,
                  Lai-Sum Yim, Sheng-Wei Huang, MingSian R. Bai, and
                  Hsueh-Cheng Wang(*Equal Contribution)
                </p>
                <p>
                  We developed a heterogeneous team of unmanned ground vehicles and blimp robots for navigating unknown
                  subterranean environments in search and rescue missions.
                  Our system integrates novel millimeter-wave radar for perception and employs deep reinforcement
                  learning for cross-modal representations.
                  Navigation is facilitated by simulation-trained deep neural networks, ensuring effective collision
                  avoidance and goal-directed movement.
                  Our communication system combines mesh WiFi, XBee, and UWB modules, alongside deployable spherical
                  nodes and miniature cars for mobile communication.
                  Field tests were conducted to evaluate propagation and signal strength indices, addressing challenges
                  such as non-line-of-sight propagation and fading reception.
                </p>
                <a class="btn btn-primary" href="https://doi.org/10.55417/fr.2022020">Paper
                </a>
                <a class="btn btn-primary" href="https://vimeo.com/501143422">Video
                </a>
                <a class="btn btn-primary" href="https://arg-nctu.github.io/SubT/">Team
                </a>
                <!-- <a class="btn btn-primary" href="project-subt-urban.html"
                    >More...</a
                  > -->
              </div>
            </div>
          </div>
        </div>

        <div class="card" id="mmWave">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/projects/deeprl-mmWave.html">
                  <figure class="cc-effect">
                    <img src="project/mmwave/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>
                    Cross-Modal Contrastive Learning of Representations for
                    Navigation Using Lightweight, Low-Cost Millimeter Wave
                    Radar for Adverse Environmental Conditions</b>
                </p>
                <p class="category">
                  IEEE Robotics and Automation Letters - 2021
                </p>
                <p>
                  Jui-Te, Huang</b>, Chen-Lung Lu, Po-Kai Chang, <b>Ching-I
                    Huang</b>, Chao-Chun Hsu, Po-Jui Huang, and Hsueh-Cheng Wang
                </p>
                <p>
                  We advocate for utilizing single-chip millimeter-wave (mmWave) radar as robot perception, particularly
                  in challenging environments such as fog, smoke, or dust.
                  To address the inherent noise and sparsity of mmWave radar signals, we propose a cross-modal
                  contrastive learning for representation (CM-CLR) method.
                  This method aims to maximize the agreement between mmWave radar data and LiDAR data during training,
                  thereby enabling autonomous navigation using radar signals.
                </p>

                <a class="btn btn-primary" href="https://ieeexplore.ieee.org/document/9362209">Paper
                </a>
                <a class="btn btn-primary" href="https://vimeo.com/468839536">Video
                </a>
                <a class="btn btn-primary" href="https://github.com/cihuang123/radar-navigation">Github
                </a>
                <!-- <a class="btn btn-primary" href="project-mmwave.html"
                    >More...</a
                  > -->
              </div>
            </div>
          </div>
        </div>

        <div class="card" id="blindguide">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/projects/guiding_robot_uwb.html">
                  <figure class="cc-effect">
                    <img src="project/blind-guide/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>Assistive Navigation Using Deep Reinforcement Learning
                    Guiding Robot With UWB/Voice Beacons and Semantic
                    Feedbacks for Blind and Visually Impaired People</b>
                </p>
                <p class="category">Frontier in Robotics and AI - 2021</p>
                <p>
                  Chen-Lung Lu, Zi-Yan Liu, Jui-Te Huang, <b>Ching-I
                    Huang</b>, Bo-Hui Wang, Yi Chen, Nien-Hsin Wu, Hsueh-Cheng Wang,
                  Laura Giarré, Pei-Yi Kuo
                </p>
                <p>
                  Facilitating navigation in pedestrian environments is
                  critical for enabling people who are blind and visually
                  impaired (BVI) to achieve independent mobility. A
                  deep-reinforcement-learning-based assistive guiding robot
                  with ultrawide-bandwidth (UWB) beacons that can navigate
                  through routes with designated waypoints was designed in
                  this study.
                </p>

                <a class="btn btn-primary"
                  href="https://www.frontiersin.org/articles/10.3389/frobt.2021.654132/full">Paper
                </a>
                <!-- <a class="btn btn-primary" href="https://arg-nctu.github.io/projects/guiding_robot_uwb.html">Web
                </a> -->
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>

    <br><br><br><br>

    <div class="section" id="conference">
      <div class="container">
        <div class="h4 text-center mb-4 title">Conference Paper</div>

        <div class="card" id="wfh-vr">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="https://arg-nctu.github.io/projects/vr-robot-arm.html">
                  <figure class="cc-effect">
                    <img src="project/wfh-vr/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>
                    Wfh-vr: Teleoperating a robot arm to set a dining table across the globe via virtual reality
                  </b>
                </p>
                <p class="category">
                  IEEE/RSJ International Conference on Intelligent Robots and
                  Systems - 2022
                </p>
                <p>
                  Lai Sum Yim, Quang TN Vo, <b>Ching-I Huang</b>, Chi-Ruei Wang,
                  Wren McQueary, Hsueh-Cheng Wang, Haikun Huang and Lap-Fai Yu
                </p>
                <p>
                  This paper introduces an accessible virtual reality-based teleoperation system for controlling a
                  low-cost robot arm (LoCoBot) using a consumer-grade VR device (Oculus Quest 2).
                  our Work-from-Home Virtual Reality (WFH-VR) system offers a seamless connection between the user
                  and the remote robot arm.
                  Virtual representations of the robot and objects are streamed to the VR interface, facilitating
                  manipulation tasks with or without network latency.
                  User studies demonstrate the system's effectiveness, enabling novices to perform dexterous
                  manipulation tasks beyond traditional keyboard controls.
                </p>

                <a class="btn btn-primary" href="https://ieeexplore.ieee.org/abstract/document/9981729">Paper
                </a>
                
              </div>
            </div>
          </div>
        </div>


      </div>
    </div>


    <br><br><br><br>

    <div class="section" id="Workshop-Competitions">
      <div class="container">
        <div class="h4 text-center mb-4 title">Workshop & Competitionsr</div>

        <div class="card" id="robotx2022">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="project/robotx2022/robotx2022.html">
                  <figure class="cc-effect">
                    <img src="project/robotx2022/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>

            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>A Learning- based modular heterogeneous usv and uav team in the maritime robotx 2022
                    competition</b>
                </p>
                <p class="category">
                  RobotX Maritime Challenge - 2022

                </p>
                <p>
                  Po-Jui Huang, <b>Ching-I Huang</b>, Sin-Kiat Lim, Po-Jui Huang, Ming-Fong Hsieh, Lai Sum Yim,
                  Yu-Ting Ko, Hao-Yang Hung, Yi Chen, Jie-Xin Liu,Li-Wei Liou, Sun-Fu Chou, Yi-Chen Teng, Kai-Rui
                  Weng, Wang-Chuan Lu, and Hsueh-Cheng Wang
                </p>
                <p>
                  Our exploration team, consisting of a USV and UAV with sensor arrays and autonomous modules, underwent refinement via hardware-in-the-loop (HIL) simulations. 
                  Utilizing Gazebo from Virtual RobotX (VRX), we developed a perception dataset for deep reinforcement learning algorithms in maritime environments. 
                  Results demonstrated the effectiveness of these algorithms, surpassing models trained solely with a UGV. 
                  We enhanced fleet management with a VR interface and developed behavior trees for task execution. 
                  Quantitative assessments were conducted in simulations before deployment on a WAM-V platform.
                </p>
                <a class="btn btn-primary" href="https://vimeo.com/758819296">Video
                </a>
                <a class="btn btn-primary" href="https://arg-nctu.github.io/robotx-2022/">
                  Team
                </a>
                <!-- <a class="btn btn-primary" href="project-robotx2022.html"
              >More...</a
            >-->
              </div> 
            </div>
          </div>
        </div>

        <div class="card" id="moos2022">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="project/moos2022/moos2022.html">
                  <figure class="cc-effect">
                    <img src="project/moos2022/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body"> 
                <p>
                  <b>Duckiepond 2.0: an Education and Research Environment of Reinforcement Learning-based
                    Navigation and
                    Virtual Reality Human Interactions for a Heterogenous Maritime Fleet</b>
                </p>
                <p class="category">
                  Moos-DAWG'22 - 2022

                </p>
                <p>
                  <b>Ching-I Huang</b>, Hao-Yang Hung, Lai Sum Yim, Quang TN Vo,
                  Lap-Fai Yu and Hsueh-Cheng Wang
                </p>
                <p>
                  Duckiepond 2.0 expands on the Duckiepond education and research environment by introducing the
                  Duckieboat platform and simulation environments in Gazebo and Unity.
                  Duckieboat (DBT22) utilizes inflatable boats and outboard motors, featuring modular designs for
                  sensor towers, autonomy boxes, and communication modules.
                  These components can be deployed on various autonomous surface vehicles, enabling longer
                  communication ranges and hardware-in-the-loop (HIL) developments.
                  Duckiepond 2.0 inherits Duckietown's fleet management capabilities and supports MOOS-ROS bridge,
                  PyIvP, and non-ROS WebSocket for autonomy education.
                  Gazebo simulations aid in developing deep reinforcement learning algorithms for collision
                  avoidance, navigation, and docking.
                  Additionally, Duckiepond 2.0 introduces "supervised autonomy" through VR interactions, allowing a
                  human supervisor to control the maritime fleet using a consumer-grade VR device (Oculus Quest 2).
                  This setup facilitates search and rescue missions with virtual representations of Duckieboats and
                  surrounding objects for streamlined operation.
                </p>
                
                <!-- <a class="btn btn-primary" href="project-robotx2022.html"
              >More...</a
            >-->
              </div> 
            </div>
          </div>
        </div>

        <div class="card" id="robotx2019">
          <div class="row">
            <div class="col-md-4" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body gallery">
                <a href="project/robotx2019/robotx2019.html">
                  <figure class="cc-effect">
                    <img src="project/robotx2019/teaser.png" alt="Image" />
                  </figure>
                </a>
              </div>
            </div>
            <div class="col-md-8" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
              <div class="card-body">
                <p>
                  <b>Long-term Operation, Low-Cost, Multi-Domain Collaborative Autonomous Maritime and Aerial
                    Platforms via Duckieboat and Duckiefloat
                  </b>
                </p>
                <p class="category">
                  RobotX Interactive Forum 2019 - 1st Place Spotlight Poster

                </p>
                <p>
                  <b>Ching-I Huang*</b>, Li-Wen Chiu*, Chao-Chun Hsu, Kuan-Lin Chen, Chih-Chuan Chang, Yi Chen, and
                  Hsueh-Cheng Wang
                </p>
                <p>
                  Duckieboat serves as the core of the Duckiepond development environment, facilitating collaborative maritime autonomy for both homogeneous and heterogeneous maritime vehicles. 
                  Our research demonstrates multi-vehicle behaviors in track-and-trail tasks, advancements in obstacle avoidance utilizing classic and learning-based approaches. 
                  Additionally, we enhance capabilities across multiple domains with Duckiefloat, an autonomous blimp developed for the DARPA Subterranean Challenge, 
                  optimized for long-term observation tasks with minimal power consumption to excute long-period observation tasks.
                </p>
              
                <!-- <a class="btn btn-primary" href="project-robotx2022.html"
              >More...</a
            >-->
              </div> 
            </div>
          </div>
        </div>



      </div>
    </div>

    <br><br><br><br>
    
    <div class="section" id="contact">
      <div
        class="cc-contact-information"
        style="background-image: url('images/background.png')"
      >
        <div class="container">
          <div class="cc-contact">
            <div class="row">
              <div class="col-md-12">
                <div class="card mb-0" data-aos="zoom-in">
                  <div class="h4 text-center title">Contact Me</div>

                  <div class="card-body">
                    <form action="https://formspree.io/moqklrpw" method="POST">
                      <div class="p pb-3">
                        <strong>Feel free to contact me </strong>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="input-group">
                            <span class="input-group-addon"
                              ><i class="fa fa-user-circle"></i
                            ></span>
                            <input
                              class="form-control"
                              type="text"
                              name="name"
                              placeholder="Name"
                              required="required"
                            />
                          </div>
                        </div>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="input-group">
                            <span class="input-group-addon"
                              ><i class="fa-solid fa-user"></i
                            ></span>
                            <input
                              class="form-control"
                              type="text"
                              name="Subject"
                              placeholder="Subject"
                              required="required"
                            />
                          </div>
                        </div>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="input-group">
                            <span class="input-group-addon"
                              ><i class="fa fa-envelope"></i
                            ></span>
                            <input
                              class="form-control"
                              type="email"
                              name="_replyto"
                              placeholder="E-mail"
                              required="required"
                            />
                          </div>
                        </div>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="form-group">
                            <textarea
                              class="form-control"
                              name="message"
                              placeholder="Your Message"
                              required="required"
                            ></textarea>
                          </div>
                        </div>
                      </div>
                      <div class="row">
                        <div class="col">
                          <button class="btn btn-primary" type="submit">
                            Send
                          </button>
                        </div>
                      </div>
                    </form>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>




  <footer class="footer">
    <div class="container text-center">
      <a class="cc-github btn btn-link" href="https://github.com/cihuang123"><i class="fa fa-github fa-2x"
          aria-hidden="true"></i></a>
      <a class="cc-linkedin btn btn-link" href="https://www.linkedin.com/in/ching-i-h-6696a9220/"><i
          class="fa fa-linkedin fa-2x" aria-hidden="true"></i></a>
    </div>
    <div class="h4 title text-center">Ching-I (Isabella) Huang</div>
    <div class="text-center text-muted">
      <p>
        © Creative CV. All rights reserved.<br />Design -
        <a class="credit" href="https://templateflip.com" target="_blank">
          TemplateFlip
        </a>
      </p>
    </div>
  </footer>
  <script src="js/core/jquery.3.2.1.min.js"></script>
  <script src="js/core/popper.min.js"></script>
  <script src="js/core/bootstrap.min.js"></script>
  <script src="js/now-ui-kit.js?v=1.1.0"></script>
  <script src="js/aos.js"></script>
  <script src="scripts/main.js"></script>
</body>

</html>